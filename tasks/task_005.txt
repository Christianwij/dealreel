# Task ID: 5
# Title: Develop Document Parsing Pipeline
# Status: done
# Dependencies: 3, 4
# Priority: high
# Description: Create a Python-based parsing service that extracts structured content from uploaded documents (PDF, PPTX, DOCX) including text, tables, metrics, and metadata.
# Details:
1. Set up a FastAPI service on Render:
```python
from fastapi import FastAPI, HTTPException, BackgroundTasks
from pydantic import BaseModel
import os

app = FastAPI()

class DocumentRequest(BaseModel):
    upload_id: str
    file_path: str
    file_type: str

@app.post("/parse-document")
async def parse_document(request: DocumentRequest, background_tasks: BackgroundTasks):
    background_tasks.add_task(process_document, request)
    return {"status": "processing", "upload_id": request.upload_id}
```

2. Install document parsing libraries:
```bash
pip install PyMuPDF pdfminer.six python-docx python-pptx pandas
```

3. Implement parsers for each document type:
```python
def process_document(request: DocumentRequest):
    # Download file from Supabase Storage
    file_path = download_from_storage(request.file_path)
    
    # Parse based on file type
    if request.file_type == 'application/pdf':
        parsed_content = parse_pdf(file_path)
    elif request.file_type == 'application/vnd.openxmlformats-officedocument.presentationml.presentation':
        parsed_content = parse_pptx(file_path)
    elif request.file_type == 'application/vnd.openxmlformats-officedocument.wordprocessingml.document':
        parsed_content = parse_docx(file_path)
    else:
        raise ValueError(f"Unsupported file type: {request.file_type}")
    
    # Clean and structure the content
    structured_content = structure_content(parsed_content)
    
    # Store parsed content in database
    store_parsed_content(request.upload_id, structured_content)
    
    # Trigger script generation
    trigger_script_generation(request.upload_id)

def parse_pdf(file_path):
    # Use PyMuPDF and pdfminer for text, tables, and structure
    import fitz  # PyMuPDF
    
    doc = fitz.open(file_path)
    content = []
    
    for page_num in range(len(doc)):
        page = doc.load_page(page_num)
        text = page.get_text("dict")
        tables = extract_tables_from_pdf_page(page)
        images = extract_images_from_pdf_page(page)
        
        content.append({
            "page_num": page_num + 1,
            "text": text,
            "tables": tables,
            "images": images
        })
    
    return content

# Similar functions for parse_pptx and parse_docx
```

4. Implement content structuring and normalization:
```python
def structure_content(parsed_content):
    # Extract sections, headings, and key metrics
    sections = extract_sections(parsed_content)
    metrics = extract_metrics(parsed_content)
    business_model = extract_business_model(parsed_content)
    
    return {
        "sections": sections,
        "metrics": metrics,
        "business_model": business_model,
        "raw_content": parsed_content
    }
```

5. Create database functions to store parsed content
6. Implement error handling and logging
7. Set up monitoring for the parsing service

# Test Strategy:
1. Unit test each parser with sample documents
2. Test extraction accuracy with known content
3. Benchmark parsing performance with various file sizes
4. Test error handling with malformed documents
5. Integration test with the upload flow
6. Verify structured output format consistency across document types

# Subtasks:
## 1. Set up FastAPI service infrastructure [done]
### Dependencies: None
### Description: Create the foundation for the document parsing service using FastAPI
### Details:
Implement the FastAPI application structure with appropriate routing, dependency injection, and configuration management. Set up the project structure, virtual environment, and required dependencies. Create endpoints for document upload and processing. Configure CORS, authentication, and request validation. Implement basic health check and service status endpoints.
<info added on 2025-05-16T21:32:43.648Z>
Implement the FastAPI application structure with appropriate routing, dependency injection, and configuration management. Set up the project structure, virtual environment, and required dependencies. Create endpoints for document upload and processing. Configure CORS, authentication, and request validation. Implement basic health check and service status endpoints.

The FastAPI document parsing service has been implemented with the following components:

1. FastAPI Application (main.py):
- Set up FastAPI with CORS middleware and Prometheus metrics
- Implemented endpoints for document parsing and status checking
- Added background task processing for asynchronous parsing
- Configured logging and monitoring

2. Document Parsers (parsers.py):
- Created parsers for PDF (PyMuPDF), DOCX (python-docx), and PPTX (python-pptx)
- Implemented structured content extraction including text, tables, and images
- Added metadata extraction for each document type
- Included error handling and logging

3. Storage Integration (storage.py):
- Implemented Supabase storage integration for file handling
- Added methods for downloading files and uploading parsed content
- Created status update functionality in the database
- Implemented temporary file management and cleanup

4. Project Structure:
- Created requirements.txt with all necessary dependencies
- Added Dockerfile for containerization
- Set up logging and temporary file directories
- Configured environment variables

The service is ready for integration with the main application and provides a robust API for parsing various document types and storing the structured content in Supabase. This implementation will support the next subtask of implementing PDF parsing with text and table extraction.
</info added on 2025-05-16T21:32:43.648Z>

## 2. Implement PDF parsing with text and table extraction [done]
### Dependencies: 5.1
### Description: Develop robust PDF parsing capabilities with support for both text and tabular content
### Details:
Integrate PDF parsing libraries (e.g., PyPDF2, pdfplumber, or pdf2image with OCR). Implement text extraction with proper handling of formatting, columns, and page breaks. Develop table detection and extraction algorithms to preserve tabular structure. Handle various PDF complexities including scanned documents, encrypted files, and different PDF versions. Create utility functions for content cleanup and initial structuring.
<info added on 2025-05-16T21:37:18.227Z>
Integrate PDF parsing libraries (e.g., PyPDF2, pdfplumber, or pdf2image with OCR). Implement text extraction with proper handling of formatting, columns, and page breaks. Develop table detection and extraction algorithms to preserve tabular structure. Handle various PDF complexities including scanned documents, encrypted files, and different PDF versions. Create utility functions for content cleanup and initial structuring.

Implementation progress:
- Created a dedicated PDFParser class in pdf_parser.py with comprehensive parsing features
- Implemented intelligent table detection and extraction algorithms that preserve the tabular structure
- Added text block extraction with formatting information preservation
- Enhanced metadata extraction capabilities from PDF documents
- Implemented image extraction with associated metadata
- Developed structured content organization by pages for better document representation
- Added robust error handling and logging mechanisms
- Implemented type safety using Python type hints throughout the codebase
- Created database schema (20240516_document_parsing.sql) with tables for document uploads, parsing jobs, and extracted content
- Set up appropriate database indexes for performance optimization
- Implemented Row Level Security policies for data protection
- Configured storage buckets and policies for document management
- Added database triggers for automatic timestamp management
</info added on 2025-05-16T21:37:18.227Z>

## 3. Implement DOCX and PPTX parsing [done]
### Dependencies: 5.1
### Description: Create parsers for Microsoft Office document formats (DOCX and PPTX)
### Details:
Integrate libraries for DOCX parsing (e.g., python-docx) and PPTX parsing (e.g., python-pptx). Extract text content while preserving document structure including headings, lists, and formatting. Handle embedded images, charts, and other objects. Extract metadata from documents. Implement slide-by-slide parsing for presentations with proper handling of notes and speaker comments.
<info added on 2025-05-16T21:38:21.720Z>
Integrate libraries for DOCX parsing (e.g., python-docx) and PPTX parsing (e.g., python-pptx). Extract text content while preserving document structure including headings, lists, and formatting. Handle embedded images, charts, and other objects. Extract metadata from documents. Implement slide-by-slide parsing for presentations with proper handling of notes and speaker comments.

The PowerPoint parsing implementation has been completed with the following components:

1. PowerPoint Parser (pptx_parser.py):
- Created dedicated PptxParser class for PPTX file processing
- Implemented comprehensive metadata extraction
- Added slide master and layout information extraction
- Created shape extraction with detailed formatting:
  - Text shapes with paragraph and run-level formatting
  - Table shapes with cell-level formatting
  - Shape positioning and dimensions
  - Shape type identification
- Implemented structured content organization by slides
- Added robust error handling and logging

2. Key Features:
- Extracts and preserves slide layout information
- Maintains text formatting (bold, italic, underline, font, size, color)
- Preserves table structure with formatting
- Captures shape positioning and dimensions
- Extracts slide master layouts and placeholder information
- Provides comprehensive metadata about the presentation
- Implements type safety with Python type hints

The PowerPoint parser complements the PDF and Word document parsers, providing a complete solution for extracting structured content from various document types. The implementation maintains consistency in the output format while handling the unique aspects of PowerPoint presentations.
</info added on 2025-05-16T21:38:21.720Z>

## 4. Create content structuring and normalization system [done]
### Dependencies: 5.2, 5.3
### Description: Develop a system to normalize and structure extracted content across different document formats
### Details:
Design a unified data model for representing document content regardless of source format. Implement content normalization to handle inconsistencies in formatting, encoding, and special characters. Create hierarchical document structure with sections, subsections, and content blocks. Develop metadata extraction and standardization. Implement content classification to identify headings, paragraphs, lists, tables, and other elements. Create JSON serialization for the normalized content.
<info added on 2025-05-16T21:39:57.519Z>
Design a unified data model for representing document content regardless of source format. Implement content normalization to handle inconsistencies in formatting, encoding, and special characters. Create hierarchical document structure with sections, subsections, and content blocks. Develop metadata extraction and standardization. Implement content classification to identify headings, paragraphs, lists, tables, and other elements. Create JSON serialization for the normalized content.

The content structuring and normalization system has been integrated with a FastAPI service layer to provide API access to the document parsing pipeline. The implementation includes:

1. API Interface:
   - RESTful endpoints for document parsing (/parse)
   - Status checking endpoint (/status) for monitoring parsing progress
   - Health monitoring endpoint (/health) for service availability checks
   - CORS middleware with configurable origins for cross-domain access

2. Processing Architecture:
   - Asynchronous document processing to handle large documents without blocking
   - Background task management for tracking multiple parsing jobs
   - Error handling with detailed status reporting
   - Automatic cleanup of temporary parsed files

3. Observability Features:
   - Prometheus metrics integration for operational monitoring
   - Document type-specific parsing metrics
   - Performance timing for optimization analysis
   - Error classification and tracking
   - Configurable logging with rotation

4. Data Modeling:
   - Pydantic models for request/response validation
   - Type-safe interfaces with UUID-based job tracking
   - Structured error response format
   - Consistent JSON output format for normalized content

5. Configuration Management:
   - Environment-based configuration
   - Worker process settings for scalability
   - Logging configuration
   - Security settings for API access

This implementation connects the content normalization system to external services while maintaining the core functionality of unified content representation across document formats.
</info added on 2025-05-16T21:39:57.519Z>

## 5. Implement error handling, logging and monitoring [done]
### Dependencies: 5.4
### Description: Develop comprehensive error handling, logging, and monitoring systems for the document parsing pipeline
### Details:
Implement structured error handling with appropriate error types and messages. Create detailed logging throughout the parsing pipeline with different severity levels. Implement performance monitoring to track parsing times and resource usage. Set up alerting for critical failures. Create detailed error reports for debugging parsing issues. Implement retry mechanisms for transient failures. Add telemetry for tracking document types, sizes, and parsing success rates.

